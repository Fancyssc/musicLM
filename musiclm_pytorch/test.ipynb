{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import wraps, partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "\n",
    "from torchaudio.transforms import Spectrogram, TimeStretch, FrequencyMasking, TimeMasking\n",
    "\n",
    "from audiolm_pytorch import AudioLM\n",
    "from audiolm_pytorch.utils import AudioConditionerBase\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "from x_clip.tokenizer import tokenizer\n",
    "from vector_quantize_pytorch import ResidualVQ\n",
    "\n",
    "from einops import rearrange, repeat, reduce, pack, unpack\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from beartype.typing import List, Optional, Tuple\n",
    "from beartype import beartype\n",
    "\n",
    "\n",
    "\n",
    "# Braincog导入\n",
    "from braincog.model_zoo.base_module import BaseModule\n",
    "from braincog.base.node.node import *\n",
    "from braincog.base.connection.layer import *\n",
    "from braincog.base.strategy.surrogate import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T03:17:02.885734600Z",
     "start_time": "2024-03-05T03:16:56.286082Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def first(it):\n",
    "    return it[0]\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def round_down_nearest_multiple(n, divisor):\n",
    "    return n // divisor * divisor\n",
    "\n",
    "def Sequential(*modules):\n",
    "    return nn.Sequential(*filter(exists, modules))\n",
    "\n",
    "# decorators\n",
    "\n",
    "def once(fn):\n",
    "    called = False\n",
    "    @wraps(fn)\n",
    "    def inner(x):\n",
    "        nonlocal called\n",
    "        if called:\n",
    "            return\n",
    "        called = True\n",
    "        return fn(x)\n",
    "    return inner\n",
    "\n",
    "print_once = once(print)\n",
    "\n",
    "# tensor functions\n",
    "\n",
    "def log(t, eps = 1e-20):\n",
    "    return torch.log(t.clamp(min = eps))\n",
    "\n",
    "def l2norm(t):\n",
    "    return F.normalize(t, p = 2, dim = -1)\n",
    "\n",
    "def matrix_diag(t):\n",
    "    device = t.device\n",
    "    i, j = t.shape[-2:]\n",
    "    num_diag_el = min(i, j)\n",
    "    i_range = torch.arange(i, device = device)\n",
    "    j_range = torch.arange(j, device = device)\n",
    "    diag_mask = rearrange(i_range, 'i -> i 1') == rearrange(j_range, 'j -> 1 j')\n",
    "    diag_el = t.masked_select(diag_mask)\n",
    "    return rearrange(diag_el, '(b d) -> b d', d = num_diag_el)\n",
    "\n",
    "# 2d sinusoidal positional embedding\n",
    "# simple vit paper shows it is good enough compared to learned\n",
    "\n",
    "def posemb_sincos_2d(patches, temperature = 10000, dtype = torch.float32):\n",
    "    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
    "\n",
    "    y, x = torch.meshgrid(torch.arange(h, device = device), torch.arange(w, device = device), indexing = 'ij')\n",
    "    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n",
    "\n",
    "    omega = torch.arange(dim // 4, device = device) / (dim // 4 - 1)\n",
    "    omega = 1. / (temperature ** omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim = 1)\n",
    "    pe = pe.type(dtype)\n",
    "\n",
    "    return rearrange(pe, '(h w) d -> h w d', h = h, w = w)\n",
    "\n",
    "# biasless layernorm\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, scale = True):\n",
    "        super().__init__()\n",
    "        self.learned_gamma = nn.Parameter(torch.ones(dim)) if scale else None\n",
    "\n",
    "        self.register_buffer('gamma', torch.ones(dim), persistent = False)\n",
    "        self.register_buffer('beta', torch.zeros(dim), persistent = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, x.shape[-1:], default(self.learned_gamma, self.gamma), self.beta)\n",
    "\n",
    "# feedforward\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim = -1)\n",
    "        return F.gelu(gate) * x\n",
    "def FeedForward(dim, mult = 4, dropout = 0.):\n",
    "    dim_hidden = int(dim * mult * 2 / 3)\n",
    "\n",
    "    return nn.Sequential(\n",
    "        LayerNorm(dim),\n",
    "        nn.Linear(dim, dim_hidden * 2, bias = False),\n",
    "        GEGLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(dim_hidden, dim, bias = False)\n",
    "    )\n",
    "\n",
    "# attention\n",
    "# 可以修改成Spike版本\n",
    "class Attention(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        step=4,\n",
    "        causal = False,\n",
    "        dim_head = 64,\n",
    "        heads = 8,    # 64 * 8 = 512\n",
    "        dropout = 0.,\n",
    "        scale = 8\n",
    "    ):\n",
    "        super().__init__(step=4,encode_type='direct')\n",
    "        self.heads = heads\n",
    "        self.scale = scale\n",
    "        self.causal = causal\n",
    "        inner_dim = dim_head * heads  # 512\n",
    "\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.q_scale = nn.Parameter(torch.ones(dim_head))\n",
    "        self.k_scale = nn.Parameter(torch.ones(dim_head))\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        rel_pos_bias = None,\n",
    "        mask = None\n",
    "    ):\n",
    "        b, n, _, device = *x.shape, x.device\n",
    "\n",
    "        # prenorm\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # project for queries, keys, values\n",
    "\n",
    "        q, k, v = self.to_q(x), *self.to_kv(x).chunk(2, dim = -1)\n",
    "\n",
    "        # split for multi-headed attention\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n",
    "\n",
    "        # qk rmsnorm, technique circulating within brain used to stabilize a 22B parameter vision model training\n",
    "\n",
    "        q, k = map(l2norm, (q, k))\n",
    "        q = q * self.q_scale\n",
    "        k = k * self.k_scale\n",
    "\n",
    "        # similarities\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        if exists(rel_pos_bias):\n",
    "            sim = sim + rel_pos_bias\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = sim.shape[-2:]\n",
    "            causal_mask = torch.ones((i, j), dtype = torch.bool, device = x.device).triu(j - i + 1)\n",
    "            sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        # aggregate\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "\n",
    "        # merge heads\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        attn_dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout),\n",
    "                FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout),\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        rel_pos_bias = None,\n",
    "        mask = None,\n",
    "        return_all_layers = False\n",
    "    ):\n",
    "        layers = []\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, rel_pos_bias = rel_pos_bias, mask = mask) + x\n",
    "            x = ff(x) + x\n",
    "            layers.append(x)\n",
    "\n",
    "        if not return_all_layers:\n",
    "            return x\n",
    "\n",
    "        return x, torch.stack(layers[:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T03:17:25.273814500Z",
     "start_time": "2024-03-05T03:17:25.254299900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "x = torch.rand(1,512,256)\n",
    "transformer = Transformer(dim=512,depth=1)\n",
    "ln = nn.LayerNorm(512)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T03:48:09.176254500Z",
     "start_time": "2024-03-05T03:48:09.162255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 512, 512]),\n tensor([[[False, False, False,  ..., False, False, False],\n          [False, False, False,  ..., False, False, False],\n          [False, False, False,  ..., False, False, False],\n          ...,\n          [False, False, False,  ..., False, False, False],\n          [False, False, False,  ..., False, False, False],\n          [False, False, False,  ..., False, False, False]]]))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = transformer(x)\n",
    "# r = ln(x)\n",
    "r.shape,r==x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T03:48:17.798090900Z",
     "start_time": "2024-03-05T03:48:17.769969800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "rerange = Rearrange('b (h p1) (w p2) -> b h w (p1 p2)', p1 = 16, p2 = 16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T03:49:05.635367700Z",
     "start_time": "2024-03-05T03:49:05.624358900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "x = rerange(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T03:49:13.960852500Z",
     "start_time": "2024-03-05T03:49:13.949842900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 32, 32, 256])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T03:49:16.587031400Z",
     "start_time": "2024-03-05T03:49:16.570690400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
